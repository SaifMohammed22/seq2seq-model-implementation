{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d57526c",
   "metadata": {},
   "source": [
    "Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0050897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd13121",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddb1227",
   "metadata": {},
   "source": [
    "Data Stuff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f1567a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS = 0\n",
    "EOS = 1\n",
    "\n",
    "class Lang():\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2idx = {}\n",
    "        self.word2count = {}\n",
    "        self.idx2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.idx2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8b6d6b",
   "metadata": {},
   "source": [
    "Convert Unicode (U+0041) -> ASCII (65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0337b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def UnicodeToAscii(s):\n",
    "#     if any('\\u0600' <= char <= '\\u06FF' or '\\u0750' <= char <= '\\u077F' \n",
    "#            or '\\u08A0' <= char <= '\\u08FF' for char in s):\n",
    "#         unwanted_categories = ['Cf', 'Cc0']\n",
    "#         return ''.join(c for c in s if unicodedata.category(c) not in unwanted_categories)\n",
    "#     else:\n",
    "#         return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "#                     if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# def normalizeString(s):\n",
    "#     s = UnicodeToAscii(s.strip().lower())\n",
    "#     s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "#     s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "#     return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1bf16ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def UnicodeToAscii(s):\n",
    "    \"\"\"Handle both Latin and Arabic scripts properly\"\"\"\n",
    "    # Check if text contains Arabic characters\n",
    "    if any('\\u0600' <= char <= '\\u06FF' or '\\u0750' <= char <= '\\u077F' \n",
    "           or '\\u08A0' <= char <= '\\u08FF' for char in s):\n",
    "        # For Arabic text, only remove some specific unwanted characters\n",
    "        # Keep most diacritics as they're meaningful\n",
    "        unwanted_categories = {'Cf', 'Cc'}  # Format and Control characters only\n",
    "        return ''.join(c for c in s if unicodedata.category(c) not in unwanted_categories)\n",
    "    else:\n",
    "        # For Latin scripts, use the original method\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                      if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = UnicodeToAscii(s.strip().lower())\n",
    "    \n",
    "    # For Arabic, be more careful with punctuation\n",
    "    if any('\\u0600' <= char <= '\\u06FF' for char in s):\n",
    "        # Arabic-specific normalization\n",
    "        s = re.sub(r'([.!?؟،])', r' \\1', s)  # Added Arabic punctuation ؟ and ،\n",
    "        # Keep Arabic letters, numbers, and basic punctuation\n",
    "        s = re.sub(r'[^\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FFa-zA-Z0-9!?؟،\\s]+', r' ', s)\n",
    "    else:\n",
    "        # Original method for Latin scripts\n",
    "        s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "        s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    \n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db3d29da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Arabic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12556</th>\n",
       "      <td>The mobile phone you have dialed is either swi...</td>\n",
       "      <td>الهاتف المتحرك الذي طلبته مغلق أو خارج نطاق ال...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12557</th>\n",
       "      <td>If you decide to answer questions now without ...</td>\n",
       "      <td>إذا قررت الإجابة عن الأسئلة الآن دون حضور محام...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12558</th>\n",
       "      <td>A man touched down on the moon. A wall came do...</td>\n",
       "      <td>هبط إنسان على سطح القمر، وأنهار حائط في برلين،...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12559</th>\n",
       "      <td>Ladies and gentlemen, please stand for the nat...</td>\n",
       "      <td>سيداتي و سادتي ، رجاءً قفوا للنشيد الوطني للات...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12560</th>\n",
       "      <td>There are mothers and fathers who will lie awa...</td>\n",
       "      <td>وهناك أمهات وآباء سيظلون مستيقظين بعد أن ينام ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 English  \\\n",
       "12556  The mobile phone you have dialed is either swi...   \n",
       "12557  If you decide to answer questions now without ...   \n",
       "12558  A man touched down on the moon. A wall came do...   \n",
       "12559  Ladies and gentlemen, please stand for the nat...   \n",
       "12560  There are mothers and fathers who will lie awa...   \n",
       "\n",
       "                                                  Arabic  \n",
       "12556  الهاتف المتحرك الذي طلبته مغلق أو خارج نطاق ال...  \n",
       "12557  إذا قررت الإجابة عن الأسئلة الآن دون حضور محام...  \n",
       "12558  هبط إنسان على سطح القمر، وأنهار حائط في برلين،...  \n",
       "12559  سيداتي و سادتي ، رجاءً قفوا للنشيد الوطني للات...  \n",
       "12560  وهناك أمهات وآباء سيظلون مستيقظين بعد أن ينام ...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/eng-ara.txt', sep='\\t', header=None, names=['English', 'Arabic'], encoding='utf-8')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1fedd557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLang(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines....\")  \n",
    "    lines = open(f\"data/{lang1}-{lang2}.txt\").read().strip().split(\"\\n\")\n",
    "    \n",
    "    pairs = [[normalizeString(s) for s in l.split(\"\\t\")] for l in lines]\n",
    "    \n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)   \n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10b047dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefix = (\n",
    "    \"i am\", \"i m\",\n",
    "    \"he is\", \"he s\",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re\",\n",
    "    \"we are\", \"we re\",\n",
    "    \"they are\", \"they re\",\n",
    ")\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(\" \")) < MAX_LENGTH and \\\n",
    "           len(p[1].split(\" \")) < MAX_LENGTH and \\\n",
    "           p[1].startswith(eng_prefix)\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3b0b0d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines....\n",
      "Read 12561 sentence pairs\n",
      "Trimmed to 1026 sentence pairs\n",
      "Counting Words...\n",
      "Counted Words:\n",
      "ara 1534\n",
      "eng 859\n",
      "['أنا امرأة قوية', 'i m a strong woman']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLang(lang1, lang2, reverse)\n",
    "    print(f\"Read {len(pairs)} sentence pairs\")\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(f\"Trimmed to {len(pairs)} sentence pairs\")\n",
    "    print(\"Counting Words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted Words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData(\"eng\", \"ara\", True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddd846d",
   "metadata": {},
   "source": [
    "--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae61fa4",
   "metadata": {},
   "source": [
    "Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "983753a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        embedded = self.embedding(input)\n",
    "        embedded = self.dropout(embedded)\n",
    "        output, hidden = self.lstm(embedded)\n",
    "        return output, hidden\n",
    "        \n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze(-1).detach()\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden, None\n",
    "    \n",
    "\n",
    "    def forward_step(self, input, hidden):\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4c1eb7",
   "metadata": {},
   "source": [
    "---\n",
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "76abf504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2idx[word] for word in sentence.split(\" \")]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "def tensorFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "def get_dataloader(batch_size):\n",
    "    input_lang, output_lang, paris = prepareData('eng', 'fra', True)\n",
    "\n",
    "    n = len(pairs)\n",
    "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "\n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        inp_ids = indexesFromSentence(input_lang, inp)\n",
    "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "        inp_ids.append(EOS)\n",
    "        tgt_ids.append(EOS)\n",
    "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, : len(tgt_ids)] = tgt_ids\n",
    "    \n",
    "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device), torch.LongTensor(target_ids).to(device))\n",
    "\n",
    "\n",
    "    train_sample = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sample, batch_size=batch_size)\n",
    "    return input_lang, output_lang, train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "050b0c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 5000])\n",
      "torch.Size([320, 5000])\n"
     ]
    }
   ],
   "source": [
    "decoder = torch.rand(32, 10, 5000)\n",
    "print(decoder.shape)\n",
    "new_decoder = decoder.view(-1, decoder.size(-1))\n",
    "print(new_decoder.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e044b62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    total_loss = 0\n",
    "\n",
    "    for data in dataloader:\n",
    "        input_tensor, target_tensor= data\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "        # Most loss function expect:\n",
    "        # Prediction: (N, C) where N = the number of samples, C = number of classes\n",
    "        # Target/Ground truth: (N,) where N = number of samples\n",
    "        # decoder_outputs: (batch_size, seq_len, vocab_len) -> (batch_size * seq_len, vocab_len)\n",
    "        # target_tensor: (batch_size, seq_len)-> (batch_size * seq_len,)\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a846afad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "af97f3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, encoder, decoder, epochs, lr=0.001, print_every=100, plot_every=100):\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0\n",
    "    plot_loss_total = 0\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print(f\"Epoch {epoch:3d}/{epochs} ({epoch/epochs*100:5.1f}%) | Avg Loss: {print_loss_avg:.4f}\")\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e286e225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, decoder_hidden, _ = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_id = topi.squeeze()\n",
    "\n",
    "        decoded_words = []\n",
    "\n",
    "        for idx in decoded_id:\n",
    "            if idx.item() == EOS:\n",
    "                decoded_words.append(\"<EOS>\")\n",
    "                break\n",
    "            decoded_words.append(output_lang.idx2word[idx.item()])\n",
    "\n",
    "        return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a261ace6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print(\">\", pair[0])\n",
    "        print(\"=\", pair[1])\n",
    "        output_words = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        output_sentence = \" \".join(output_words)\n",
    "        print(\"<\", output_sentence)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fbbf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines....\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 12892 sentence pairs\n",
      "Counting Words...\n",
      "Counted Words:\n",
      "fra 5228\n",
      "eng 3434\n",
      "Epoch   5/80 (  6.2%) | Avg Loss: 2.2408\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "batch_size = 32\n",
    "\n",
    "input_lang, output_lang, train_loader = get_dataloader(batch_size)\n",
    "\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "\n",
    "train(train_loader, encoder, decoder, epochs=80, print_every=5, plot_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fab2dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> je suis en vacances\n",
      "= i m on holiday\n",
      "< i m the one <EOS>\n",
      "\n",
      "> il est cense etre chez lui aujourd hui\n",
      "= he is supposed to be at home today\n",
      "< he is used to be in to <EOS>\n",
      "\n",
      "> il me faut trouver de nouvelles amies\n",
      "= i must find some new friends\n",
      "< i must find new new new <EOS>\n",
      "\n",
      "> vous avez tort\n",
      "= you are wrong\n",
      "< you re wrong <EOS>\n",
      "\n",
      "> elle est une vrai beaute\n",
      "= she is a real beauty\n",
      "< she is a doctor beautiful <EOS>\n",
      "\n",
      "> nous sommes pareils\n",
      "= we re the same\n",
      "< we re the here <EOS>\n",
      "\n",
      "> j ai emis des reserves\n",
      "= i made reservations\n",
      "< i made you <EOS>\n",
      "\n",
      "> je ne suis toujours pas impressionne\n",
      "= i m still not impressed\n",
      "< i m not not either <EOS>\n",
      "\n",
      "> nous sommes tres reconnaissants pour votre hospitalite\n",
      "= we re very grateful for your hospitality\n",
      "< we re very grateful for your hospitality <EOS>\n",
      "\n",
      "> il tira sur l oiseau mais le manqua\n",
      "= he shot at the bird but missed it\n",
      "< he saw the the bird the the <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "evaluateRandomly(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6f095b",
   "metadata": {},
   "source": [
    "----\n",
    "Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd92a5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/saif/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1934633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_blue(encoder, decoder, pairs, input_lang, output_lang, n_samples=100):\n",
    "    reference = []\n",
    "    hypotheses = []\n",
    "\n",
    "    test_paris = random.sample(pairs, min(n_samples, len(pairs)))\n",
    "\n",
    "    for pair in test_paris:\n",
    "        reference = pair[1].split() #target\n",
    "        reference.append([reference])\n",
    "\n",
    "        predicted_words = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        if \"<EOS>\" in predicted_words:\n",
    "            predicted_words = predicted_words[:predicted_words.index(\"<EOS>\")]\n",
    "        hypotheses.append(predicted_words)\n",
    "    \n",
    "    smoothie = SmoothingFunction().method4\n",
    "    blue_score = corpus_bleu(reference, hypotheses, smoothing_function=smoothie)\n",
    "\n",
    "    return blue_score * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df93a4d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 60])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.randn((10,20))\n",
    "B = torch.randn((10,20))\n",
    "C = torch.randn((10,20))\n",
    "\n",
    "\n",
    "W = torch.hstack([A, B, C])\n",
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f44feb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
